{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import model_runner\n",
    "import embeddings\n",
    "import dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reader to get training and test data from csv\n",
    "reader = dataset.csv_reader()\n",
    "\n",
    "# Open train.csv\n",
    "train_path = os.path.join(os.path.curdir, \"dataset\", \"train.csv\")\n",
    "reader.open_csv(train_path)\n",
    "train_reviews = reader.read(-1)\n",
    "\n",
    "# And test.csv\n",
    "test_path = os.path.join(os.path.curdir, \"dataset\", \"test.csv\")\n",
    "reader.open_csv(test_path)\n",
    "test_reviews = reader.read(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding text reviews to vector representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize the embedder to use the glove-wiki-gigaword-50 embedding dictionary\n",
    "# https://github.com/piskvorky/gensim-data#:~:text=org/licenses/pddl/-,glove%2Dwiki%2Dgigaword%2D50,-400000\n",
    "review_embedder = embeddings.review_embedder()\n",
    "review_embedder.load_embedding_model(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dict denotes how review labels (strings) should be mapped to one-hot encodings (tensors)\n",
    "# Note that this mapping is for the original dataset,the dataset with remapped labels will look different\n",
    "review_label_mapping = {\n",
    "    \"1\": torch.tensor([1., 0., 0., 0., 0.]),\n",
    "    \"2\": torch.tensor([0., 1., 0., 0., 0.]),\n",
    "    \"3\": torch.tensor([0., 0., 1., 0., 0.]),\n",
    "    \"4\": torch.tensor([0., 0., 0., 1., 0.]),\n",
    "    \"5\": torch.tensor([0., 0., 0., 0., 1.]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating final embeddings\n",
    "# Based on this dict and the embedding scheme selected above (glove-wiki-gigaword-50),\n",
    "# we can embed our review text and labels to tensors\n",
    "# \n",
    "# Note that there are a few extra parameters to the embedder which aren't shown below:\n",
    "# \n",
    "# oov_feature: creates an extra label feature which is zero usually, except for when a word\n",
    "#   of a review can't be embedded because it is not contained in the word vector list\n",
    "#   of the chosen embedding scheme (the word is then \"out-of-vocab\"). When an OOV word\n",
    "#   is encountered in the review text and oov_feature is True (default), then the resulting\n",
    "#   word vector is a bunch of zeroes, plus a one in the oov_feature postition; when\n",
    "#   this happens and oov_feature is False, the word is simply skipped.\n",
    "# \n",
    "# title_body_feature: similar to oov_feature, this creates an extra label feature which is\n",
    "#   zero for words appearing in the title of the review and one for words appearing in the\n",
    "#   body of the review.\n",
    "# train_features, train_labels = review_embedder.embed_dataset_features_and_labels(train_reviews, review_label_mapping)\n",
    "# test_features, test_labels = review_embedder.embed_dataset_features_and_labels(test_reviews, review_label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([54, 52]), torch.Size([1, 5]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New sampler, which can run on the full dataset\n",
    "# TODO desc\n",
    "train_sampler = embeddings.review_embedder_sampler(train_reviews, review_embedder, review_label_mapping)\n",
    "test_sampler = embeddings.review_embedder_sampler(test_reviews, review_embedder, review_label_mapping)\n",
    "\n",
    "# Testing the samplers\n",
    "x_sample, y_sample = next(iter(train_sampler))\n",
    "x_sample.shape, y_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch has a built-in LSTM class which does what we need\n",
    "model = nn.LSTM(input_size=52, hidden_size=100, num_layers=1, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class review_LSTM(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_classifier: nn.Module, batch_first: bool = True):\n",
    "        super().__init__()\n",
    "        self._LSTM = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=batch_first)\n",
    "        self._output_classifier = output_classifier\n",
    "        super().add_module(\"LSTM\", self._LSTM)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor | nn.utils.rnn.PackedSequence):\n",
    "        # Give the review data to the LSTM to munch on\n",
    "        output, (h_n, c_n) = self._LSTM.forward(x)\n",
    "        \n",
    "        # c_n is the cell state of the LSTM given all the data it has seen so far, and is supposed to\n",
    "        # represent the LSTM's overall interpretation of the data; it can be used as a feature vector\n",
    "        # for the output classifier to make a final class prediction. Feed it to the output classifier!\n",
    "        yhat = self._output_classifier.forward(c_n)\n",
    "        \n",
    "        # Return the results from the output classifier\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 52\n",
    "hidden_size = 100\n",
    "output_size = 5\n",
    "\n",
    "output_classifier = nn.Sequential(\n",
    "    nn.Linear(hidden_size, output_size),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "model = review_LSTM(input_size, hidden_size, output_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model optimizer objects\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model runner to handle training\n",
    "runner = model_runner.runner(model_name=\"LSTM_test_full_dataset\", model=model, optimizer=optim, loss_fn=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 43:   0%|          | 10241/3000000 [00:51<4:10:17, 199.08batches/s, batch loss=0.914, epoch train accuracy=42.05%]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\will\\Documents\\ECE539 Final Project\\ECE539-Final-Project\\lstm_example.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/will/Documents/ECE539%20Final%20Project/ECE539-Final-Project/lstm_example.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Train that model!\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/will/Documents/ECE539%20Final%20Project/ECE539-Final-Project/lstm_example.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m runner\u001b[39m.\u001b[39;49mtrain(train_sampler, test_sampler, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, autosave_interval_epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32md:\\will\\Documents\\ECE539 Final Project\\ECE539-Final-Project\\model_runner.py:164\u001b[0m, in \u001b[0;36mrunner.train\u001b[1;34m(self, train_batch_iterator, val_batch_iterator, num_epochs, autosave_interval_epochs)\u001b[0m\n\u001b[0;32m    161\u001b[0m starting_epoch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epoch\n\u001b[0;32m    162\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m    163\u001b[0m     \u001b[39m# Train the epoch\u001b[39;00m\n\u001b[1;32m--> 164\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_epoch(train_batch_iterator)\n\u001b[0;32m    166\u001b[0m     \u001b[39m# Measure model accuracy against the validation dataset\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     val_accuracy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier_accuracy_score(val_batch_iterator)\n",
      "File \u001b[1;32md:\\will\\Documents\\ECE539 Final Project\\ECE539-Final-Project\\model_runner.py:140\u001b[0m, in \u001b[0;36mrunner._train_epoch\u001b[1;34m(self, train_batch_iterator)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(train_batch_iterator, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining Epoch \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, position\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatches\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m tepoch:\n\u001b[0;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m x_batch, y_batch \u001b[39min\u001b[39;00m tepoch:\n\u001b[0;32m    139\u001b[0m         \u001b[39m# Batch train\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m         batch_loss, batch_accurate_predictions, batch_samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_batch(x_batch, y_batch)\n\u001b[0;32m    142\u001b[0m         \u001b[39m# Update statistics\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         epoch_total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m batch_loss \u001b[39m*\u001b[39m batch_samples\n",
      "File \u001b[1;32md:\\will\\Documents\\ECE539 Final Project\\ECE539-Final-Project\\model_runner.py:109\u001b[0m, in \u001b[0;36mrunner._train_batch\u001b[1;34m(self, x_batch, y_batch)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m    108\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m yhat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model\u001b[39m.\u001b[39;49mforward(x_batch)\n\u001b[0;32m    111\u001b[0m \u001b[39m# Loss\u001b[39;00m\n\u001b[0;32m    112\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loss_fn(yhat, y_batch)\n",
      "\u001b[1;32md:\\will\\Documents\\ECE539 Final Project\\ECE539-Final-Project\\lstm_example.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/will/Documents/ECE539%20Final%20Project/ECE539-Final-Project/lstm_example.ipynb#X21sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor \u001b[39m|\u001b[39m nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mrnn\u001b[39m.\u001b[39mPackedSequence):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/will/Documents/ECE539%20Final%20Project/ECE539-Final-Project/lstm_example.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# Give the review data to the LSTM to munch on\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/will/Documents/ECE539%20Final%20Project/ECE539-Final-Project/lstm_example.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     output, (h_n, c_n) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_LSTM\u001b[39m.\u001b[39;49mforward(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/will/Documents/ECE539%20Final%20Project/ECE539-Final-Project/lstm_example.ipynb#X21sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# c_n is the cell state of the LSTM given all the data it has seen so far, and is supposed to\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/will/Documents/ECE539%20Final%20Project/ECE539-Final-Project/lstm_example.ipynb#X21sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# represent the LSTM's overall interpretation of the data; it can be used as a feature vector\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/will/Documents/ECE539%20Final%20Project/ECE539-Final-Project/lstm_example.ipynb#X21sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# for the output classifier to make a final class prediction. Feed it to the output classifier!\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/will/Documents/ECE539%20Final%20Project/ECE539-Final-Project/lstm_example.ipynb#X21sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     yhat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_classifier\u001b[39m.\u001b[39mforward(c_n)\n",
      "File \u001b[1;32md:\\will\\Documents\\ECE539 Final Project\\ECE539-Final-Project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:879\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    876\u001b[0m         hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m    878\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 879\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[0;32m    880\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[0;32m    881\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    882\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[0;32m    883\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train that model!\n",
    "runner.train(train_sampler, test_sampler, num_epochs=5, autosave_interval_epochs=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
