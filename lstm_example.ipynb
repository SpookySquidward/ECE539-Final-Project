{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import model_runner\n",
    "import embeddings\n",
    "import dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reader to get training and test data from csv\n",
    "reader = dataset.csv_reader()\n",
    "\n",
    "# Open train.csv\n",
    "train_path = os.path.join(os.path.curdir, \"dataset\", \"train_small.csv\")\n",
    "reader.open_csv(train_path)\n",
    "train_reviews = reader.read(-1)\n",
    "\n",
    "# And test.csv\n",
    "test_path = os.path.join(os.path.curdir, \"dataset\", \"test_small.csv\")\n",
    "reader.open_csv(test_path)\n",
    "test_reviews = reader.read(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding text reviews to vector representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize the embedder to use the glove-wiki-gigaword-50 embedding dictionary\n",
    "# https://github.com/piskvorky/gensim-data#:~:text=org/licenses/pddl/-,glove%2Dwiki%2Dgigaword%2D50,-400000\n",
    "review_embedder = embeddings.review_embedder()\n",
    "review_embedder.load_embedding_model(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dict denotes how review labels (strings) should be mapped to one-hot encodings (tensors)\n",
    "# Note that this mapping is for the original dataset,the dataset with remapped labels will look different\n",
    "review_label_mapping = {\n",
    "    \"1\": torch.tensor([1., 0., 0., 0., 0.]),\n",
    "    \"2\": torch.tensor([0., 1., 0., 0., 0.]),\n",
    "    \"3\": torch.tensor([0., 0., 1., 0., 0.]),\n",
    "    \"4\": torch.tensor([0., 0., 0., 1., 0.]),\n",
    "    \"5\": torch.tensor([0., 0., 0., 0., 1.]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding features: 100%|██████████| 10000/10000 [00:02<00:00, 3947.91it/s]\n",
      "Embedding features: 100%|██████████| 10000/10000 [00:02<00:00, 3899.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# Creating final embeddings\n",
    "# Based on this dict and the embedding scheme selected above (glove-wiki-gigaword-50),\n",
    "# we can embed our review text and labels to tensors\n",
    "# \n",
    "# Note that there are a few extra parameters to the embedder which aren't shown below:\n",
    "# \n",
    "# oov_feature: creates an extra label feature which is zero usually, except for when a word\n",
    "#   of a review can't be embedded because it is not contained in the word vector list\n",
    "#   of the chosen embedding scheme (the word is then \"out-of-vocab\"). When an OOV word\n",
    "#   is encountered in the review text and oov_feature is True (default), then the resulting\n",
    "#   word vector is a bunch of zeroes, plus a one in the oov_feature postition; when\n",
    "#   this happens and oov_feature is False, the word is simply skipped.\n",
    "# \n",
    "# title_body_feature: similar to oov_feature, this creates an extra label feature which is\n",
    "#   zero for words appearing in the title of the review and one for words appearing in the\n",
    "#   body of the review.\n",
    "train_features, train_labels = review_embedder.embed_dataset_features_and_labels(train_reviews, review_label_mapping)\n",
    "test_features, test_labels = review_embedder.embed_dataset_features_and_labels(test_reviews, review_label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([190, 52]), torch.Size([1, 5]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constructing a data reader for our embedded reviews\n",
    "# This is similar to a DataLoader, but we can't use that with variable-length data :(\n",
    "# NOTE: this reader implementation is too slow to run on the full dataset and needs to be updated!\n",
    "train_sampler = embeddings.embedded_review_random_sampler(train_features, train_labels)\n",
    "test_sampler = embeddings.embedded_review_random_sampler(test_features, test_labels)\n",
    "\n",
    "# Testing the sampler\n",
    "x_sample, y_sample = next(iter(train_sampler))\n",
    "x_sample.shape, y_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch has a built-in LSTM class which does what we need\n",
    "model = nn.LSTM(input_size=52, hidden_size=100, num_layers=1, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class review_LSTM(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_classifier: nn.Module, batch_first: bool = True):\n",
    "        super().__init__()\n",
    "        self._LSTM = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=batch_first)\n",
    "        self._output_classifier = output_classifier\n",
    "        super().add_module(\"LSTM\", self._LSTM)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor | nn.utils.rnn.PackedSequence):\n",
    "        # Give the review data to the LSTM to munch on\n",
    "        output, (h_n, c_n) = self._LSTM.forward(x)\n",
    "        \n",
    "        # c_n is the cell state of the LSTM given all the data it has seen so far, and is supposed to\n",
    "        # represent the LSTM's overall interpretation of the data; it can be used as a feature vector\n",
    "        # for the output classifier to make a final class prediction. Feed it to the output classifier!\n",
    "        yhat = self._output_classifier.forward(c_n)\n",
    "        \n",
    "        # Return the results from the output classifier\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 52\n",
    "hidden_size = 100\n",
    "output_size = 5\n",
    "\n",
    "output_classifier = nn.Sequential(\n",
    "    nn.Linear(hidden_size, output_size),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "model = review_LSTM(input_size, hidden_size, output_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model optimizer objects\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model runner to handle training\n",
    "runner = model_runner.runner(model_name=\"LSTM_test\", model=model, optimizer=optim, loss_fn=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 10000/10000 [00:46<00:00, 215.81it/s, batch loss=0.915, epoch train accuracy=37.0%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'most_accurate_model_epoch': 13, 'self._epoch': 14, 'epochs_since_starting_training': 1, 'self._loss_history': [1.6068969820857049, 1.5920192344069481, 1.5794373497307301, 1.5999930676281453, 1.5752094769895078, 1.6102991409838199, 1.577257874405384, 1.5670897438526155, 1.5874886572241784, 1.5601403160870075, 1.5795987164139749, 1.5588250208079815, 1.5186069102168083, 1.498301039505005], 'self._train_acc_history': [0.21902190219021903, 0.2508250825082508, 0.26752675267526754, 0.24542454245424541, 0.2737273727372737, 0.23542354235423543, 0.27262726272627263, 0.2861286128612861, 0.2591259125912591, 0.29812981298129815, 0.2765276527652765, 0.29572957295729574, 0.34493449344934496, 0.37013701370137014], 'self._val_acc_history': [0.2381, 0.2649, 0.2618, 0.2308, 0.2829, 0.2457, 0.2649, 0.2953, 0.2707, 0.3251, 0.2924, 0.3051, 0.37, 0.3407]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 10000/10000 [00:46<00:00, 214.47it/s, batch loss=1.299, epoch train accuracy=38.5%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'most_accurate_model_epoch': 15, 'self._epoch': 15, 'epochs_since_starting_training': 2, 'self._loss_history': [1.6068969820857049, 1.5920192344069481, 1.5794373497307301, 1.5999930676281453, 1.5752094769895078, 1.6102991409838199, 1.577257874405384, 1.5670897438526155, 1.5874886572241784, 1.5601403160870075, 1.5795987164139749, 1.5588250208079815, 1.5186069102168083, 1.498301039505005, 1.4848255165100097], 'self._train_acc_history': [0.21902190219021903, 0.2508250825082508, 0.26752675267526754, 0.24542454245424541, 0.2737273727372737, 0.23542354235423543, 0.27262726272627263, 0.2861286128612861, 0.2591259125912591, 0.29812981298129815, 0.2765276527652765, 0.29572957295729574, 0.34493449344934496, 0.37013701370137014, 0.3852385238523852], 'self._val_acc_history': [0.2381, 0.2649, 0.2618, 0.2308, 0.2829, 0.2457, 0.2649, 0.2953, 0.2707, 0.3251, 0.2924, 0.3051, 0.37, 0.3407, 0.3761]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 10000/10000 [00:46<00:00, 215.26it/s, batch loss=0.909, epoch train accuracy=40.6%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'most_accurate_model_epoch': 16, 'self._epoch': 16, 'epochs_since_starting_training': 3, 'self._loss_history': [1.6068969820857049, 1.5920192344069481, 1.5794373497307301, 1.5999930676281453, 1.5752094769895078, 1.6102991409838199, 1.577257874405384, 1.5670897438526155, 1.5874886572241784, 1.5601403160870075, 1.5795987164139749, 1.5588250208079815, 1.5186069102168083, 1.498301039505005, 1.4848255165100097, 1.4713312471032143], 'self._train_acc_history': [0.21902190219021903, 0.2508250825082508, 0.26752675267526754, 0.24542454245424541, 0.2737273727372737, 0.23542354235423543, 0.27262726272627263, 0.2861286128612861, 0.2591259125912591, 0.29812981298129815, 0.2765276527652765, 0.29572957295729574, 0.34493449344934496, 0.37013701370137014, 0.3852385238523852, 0.40594059405940597], 'self._val_acc_history': [0.2381, 0.2649, 0.2618, 0.2308, 0.2829, 0.2457, 0.2649, 0.2953, 0.2707, 0.3251, 0.2924, 0.3051, 0.37, 0.3407, 0.3761, 0.3989]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17:   3%|▎         | 314/10000 [00:01<00:48, 199.42it/s, batch loss=1.747, epoch train accuracy=37.1%]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\will\\Documents\\ECE539 Final Project\\ECE539-Final-Project\\lstm_example.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/will/Documents/ECE539%20Final%20Project/ECE539-Final-Project/lstm_example.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Train that model!\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/will/Documents/ECE539%20Final%20Project/ECE539-Final-Project/lstm_example.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m runner\u001b[39m.\u001b[39;49mtrain(train_sampler, test_sampler, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, autosave_interval_epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32md:\\will\\Documents\\ECE539 Final Project\\ECE539-Final-Project\\model_runner.py:164\u001b[0m, in \u001b[0;36mrunner.train\u001b[1;34m(self, train_batch_iterator, val_batch_iterator, num_epochs, autosave_interval_epochs)\u001b[0m\n\u001b[0;32m    161\u001b[0m starting_epoch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epoch\n\u001b[0;32m    162\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m    163\u001b[0m     \u001b[39m# Train the epoch\u001b[39;00m\n\u001b[1;32m--> 164\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_epoch(train_batch_iterator)\n\u001b[0;32m    166\u001b[0m     \u001b[39m# Measure model accuracy against the validation dataset\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     val_accuracy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier_accuracy_score(val_batch_iterator)\n",
      "File \u001b[1;32md:\\will\\Documents\\ECE539 Final Project\\ECE539-Final-Project\\model_runner.py:140\u001b[0m, in \u001b[0;36mrunner._train_epoch\u001b[1;34m(self, train_batch_iterator)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(train_batch_iterator, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, position\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mas\u001b[39;00m tepoch:\n\u001b[0;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m x_batch, y_batch \u001b[39min\u001b[39;00m tepoch:\n\u001b[0;32m    139\u001b[0m         \u001b[39m# Batch train\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m         batch_loss, batch_accurate_predictions, batch_samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_batch(x_batch, y_batch)\n\u001b[0;32m    142\u001b[0m         \u001b[39m# Update statistics\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         epoch_total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m batch_loss \u001b[39m*\u001b[39m batch_samples\n",
      "File \u001b[1;32md:\\will\\Documents\\ECE539 Final Project\\ECE539-Final-Project\\model_runner.py:109\u001b[0m, in \u001b[0;36mrunner._train_batch\u001b[1;34m(self, x_batch, y_batch)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m    108\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m yhat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model\u001b[39m.\u001b[39;49mforward(x_batch)\n\u001b[0;32m    111\u001b[0m \u001b[39m# Loss\u001b[39;00m\n\u001b[0;32m    112\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loss_fn(yhat, y_batch)\n",
      "\u001b[1;32md:\\will\\Documents\\ECE539 Final Project\\ECE539-Final-Project\\lstm_example.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/will/Documents/ECE539%20Final%20Project/ECE539-Final-Project/lstm_example.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m output, (h_n, c_n) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_LSTM\u001b[39m.\u001b[39mforward(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/will/Documents/ECE539%20Final%20Project/ECE539-Final-Project/lstm_example.ipynb#X21sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# c_n is the cell state of the LSTM given all the data it has seen so far, and is supposed to\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/will/Documents/ECE539%20Final%20Project/ECE539-Final-Project/lstm_example.ipynb#X21sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# represent the LSTM's overall interpretation of the data; it can be used as a feature vector\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/will/Documents/ECE539%20Final%20Project/ECE539-Final-Project/lstm_example.ipynb#X21sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# for the output classifier to make a final class prediction. Feed it to the output classifier!\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/will/Documents/ECE539%20Final%20Project/ECE539-Final-Project/lstm_example.ipynb#X21sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m yhat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_output_classifier\u001b[39m.\u001b[39mforward(c_n)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/will/Documents/ECE539%20Final%20Project/ECE539-Final-Project/lstm_example.ipynb#X21sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Return the results from the output classifier\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/will/Documents/ECE539%20Final%20Project/ECE539-Final-Project/lstm_example.ipynb#X21sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mreturn\u001b[39;00m yhat\n",
      "File \u001b[1;32md:\\will\\Documents\\ECE539 Final Project\\ECE539-Final-Project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1682\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1673\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39m=\u001b[39m OrderedDict()\n\u001b[0;32m   1675\u001b[0m \u001b[39m# On the return type:\u001b[39;00m\n\u001b[0;32m   1676\u001b[0m \u001b[39m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[0;32m   1677\u001b[0m \u001b[39m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1680\u001b[0m \u001b[39m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[0;32m   1681\u001b[0m \u001b[39m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[1;32m-> 1682\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m   1683\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[0;32m   1684\u001b[0m         _parameters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train that model!\n",
    "runner.train(train_sampler, test_sampler, num_epochs=10, autosave_interval_epochs=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
