{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Example\n",
    "\n",
    "This notebook must be moved to the root project directory to run; it walks through all the steps involved in creating\n",
    "and training an LSTM classifier for Amazon reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import model_runner\n",
    "import embeddings\n",
    "import dataset\n",
    "import os\n",
    "from models.lstm import review_LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reader to get train and val data from csv\n",
    "reader = dataset.csv_reader()\n",
    "\n",
    "# Open formatted_train.csv\n",
    "train_path = os.path.join(os.path.curdir, \"dataset\", \"formatted_train.csv\")\n",
    "reader.open_csv(train_path, skip_header=True)\n",
    "train_reviews = reader.read(-1)\n",
    "\n",
    "# And formatted_val.csv\n",
    "val_path = os.path.join(os.path.curdir, \"dataset\", \"formatted_val.csv\")\n",
    "reader.open_csv(val_path, skip_header=True)\n",
    "val_reviews = reader.read(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding text reviews to vector representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize the embedder to use the glove-wiki-gigaword-50 embedding dictionary\n",
    "# https://github.com/piskvorky/gensim-data#:~:text=org/licenses/pddl/-,glove%2Dwiki%2Dgigaword%2D50,-400000\n",
    "review_embedder = embeddings.review_embedder(review_labels=[\"negative\", \"positive\"], embedding_model=\"glove-wiki-gigaword-50\", oov_feature=True, title_body_feature=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([4206, 52]), tensor(50), torch.Size([50, 2]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we tried to embed our entire training dataset into feature vectors at once, it would utilize over 100GB of RAM,\n",
    "# which is unrealistic for most computers. The batched_review_embedder_sampler class embeds samples a few at a time,\n",
    "# reducing RAM usage compared to a traditional DataLoader, at the expense of needing to re-embed the reviews every epoch\n",
    "train_sampler = embeddings.batched_review_embedder_sampler(train_reviews, review_embedder, batch_size=50)\n",
    "val_sampler = embeddings.batched_review_embedder_sampler(val_reviews, review_embedder, batch_size=50)\n",
    "\n",
    "# Testing the samplers\n",
    "x_sample, y_sample = next(iter(train_sampler))\n",
    "x_sample.data.shape, x_sample.batch_sizes[0], y_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "input_size = review_embedder.feature_embedding_size\n",
    "hidden_size = 100\n",
    "output_size = 2\n",
    "num_layers = 1\n",
    "\n",
    "# The output classifier makes a final prediction of the label based on the final hidden state of the LSTM; this could be\n",
    "# any feedforward architecture, but as a starting point a single linear layer is used. The output should have a softmax\n",
    "# activation in order for out cross entropy loss function to work well\n",
    "output_classifier = nn.Sequential(nn.Linear(hidden_size * num_layers, output_size), nn.Softmax(dim=1))\n",
    "\n",
    "# The full model consists of an LSTM RNN and the output classifier defined above\n",
    "model = review_LSTM(input_size, hidden_size, output_classifier, num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model optimizer objects\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model runner to handle training\n",
    "runner = model_runner.runner(model_name=\"LSTM_test\", model=model, optimizer=optim, loss_fn=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully locked PC to prevent it sleeping during training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17: 100%|██████████| 38400/38400 [14:50<00:00, 43.12batches/s, batch loss=0.423, epoch train accuracy=87.31%]\n",
      "Evaluating model predictions: 100%|██████████| 9600/9600 [03:18<00:00, 48.26batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached epoch save interval, saving model state...\n"
     ]
    }
   ],
   "source": [
    "# Train that model!\n",
    "from wakepy import keep\n",
    "\n",
    "with keep.running() as k:\n",
    "    print(\"Successfully locked PC to prevent it sleeping during training!\" if k.success else \"Wasn't able to lock PC from sleeping during training!\")\n",
    "    runner.train(train_sampler, num_epochs=1, val_batch_iterable=val_sampler, autosave_interval_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the test dataset\n",
    "test_path = os.path.join(os.path.curdir, \"dataset\", \"formatted_test.csv\")\n",
    "reader.open_csv(test_path, skip_header=True)\n",
    "test_reviews = reader.read(-1)\n",
    "test_sampler = embeddings.batched_review_embedder_sampler(test_reviews, review_embedder, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model predictions: 100%|██████████| 10400/10400 [03:32<00:00, 49.02batches/s]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's performance on the test dataset\n",
    "y_test, yhat_test = runner.predict_dataset(test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([519995])\n",
      "torch.Size([519995])\n",
      "0.8714949182203675\n"
     ]
    }
   ],
   "source": [
    "# We can now look at testing metrics using our predictions; here, we only consider accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(y_test.shape, yhat_test.shape, accuracy_score(y_test, yhat_test), sep='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
